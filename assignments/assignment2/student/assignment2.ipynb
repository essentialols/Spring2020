{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# initializing otter-grader\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Assignment-2:-Command-line-and-Data-Scraping\" data-toc-modified-id=\"Assignment-2:-Command-line-and-Data-Scraping-1\">Assignment 2: Command line and Data Scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#Due-date:-April-30,-2020-at-11:59-pm\" data-toc-modified-id=\"Due-date:-April-30,-2020-at-11:59-pm-1.1\">Due date: April 30, 2020 at 11:59 pm</a></span></li></ul></li><li><span><a href=\"#Name:\" data-toc-modified-id=\"Name:-2\">Name:</a></span></li><li><span><a href=\"#Perm-Number:\" data-toc-modified-id=\"Perm-Number:-3\">Perm Number:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collaborating-classmates:\" data-toc-modified-id=\"Collaborating-classmates:-3.1\">Collaborating classmates:</a></span></li><li><span><a href=\"#Important!\" data-toc-modified-id=\"Important!-3.2\">Important!</a></span></li></ul></li><li><span><a href=\"#Question-1:-Basics\" data-toc-modified-id=\"Question-1:-Basics-4\">Question 1: Basics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1a:-Downloading-from-URL\" data-toc-modified-id=\"Question-1a:-Downloading-from-URL-4.1\">Question 1a: Downloading from URL</a></span></li><li><span><a href=\"#Question-1b:-Chaining-commands\" data-toc-modified-id=\"Question-1b:-Chaining-commands-4.2\">Question 1b: Chaining commands</a></span></li></ul></li><li><span><a href=\"#Question-2:-Text-Processing\" data-toc-modified-id=\"Question-2:-Text-Processing-5\">Question 2: Text Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-2a:-grep\" data-toc-modified-id=\"Question-2a:-grep-5.1\">Question 2a: <code>grep</code></a></span></li><li><span><a href=\"#Question-2b:-Regular-expression\" data-toc-modified-id=\"Question-2b:-Regular-expression-5.2\">Question 2b: Regular expression</a></span></li><li><span><a href=\"#Question-2c:-Double-check\" data-toc-modified-id=\"Question-2c:-Double-check-5.3\">Question 2c: Double check</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-5.4\">Question 3</a></span></li><li><span><a href=\"#Question-3a:-Inspecting-the-Header\" data-toc-modified-id=\"Question-3a:-Inspecting-the-Header-5.5\">Question 3a: Inspecting the Header</a></span></li><li><span><a href=\"#Question-3b:-Count-Locations\" data-toc-modified-id=\"Question-3b:-Count-Locations-5.6\">Question 3b: Count Locations</a></span></li><li><span><a href=\"#Question-3c:-View-CSV-in-Table\" data-toc-modified-id=\"Question-3c:-View-CSV-in-Table-5.7\">Question 3c: View CSV in Table</a></span></li></ul></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-6\">Question 4</a></span></li><li><span><a href=\"#Question-4a:-Reading-data-in\" data-toc-modified-id=\"Question-4a:-Reading-data-in-7\">Question 4a: Reading data in</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-4b:-Subsetting-Data\" data-toc-modified-id=\"Question-4b:-Subsetting-Data-7.1\">Question 4b: Subsetting Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#DataFrame-fullaqi\" data-toc-modified-id=\"DataFrame-fullaqi-7.1.1\">DataFrame <code>fullaqi</code></a></span></li><li><span><a href=\"#DataFrame-worst_counties\" data-toc-modified-id=\"DataFrame-worst_counties-7.1.2\">DataFrame <code>worst_counties</code></a></span></li></ul></li><li><span><a href=\"#Question-4c:-Visualization\" data-toc-modified-id=\"Question-4c:-Visualization-7.2\">Question 4c: Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-Annual-AQI-Changes\" data-toc-modified-id=\"Visualize-Annual-AQI-Changes-7.2.1\">Visualize Annual AQI Changes</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Command line and Data Scraping\n",
    "\n",
    "## Due date: April 30, 2020 at 11:59 pm\n",
    "\n",
    "# Name: \n",
    "# Perm Number:\n",
    "\n",
    "## Collaborating classmates:\n",
    "\n",
    "## Important!\n",
    "\n",
    "Gradescope autograder will not work with this notebook because of the difference in how command line commands run in your notebook vs. on Gradescope. You will rely soley on the built-in notebook for feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 1: Basics\n",
    "\n",
    "## Question 1a: Downloading from URL\n",
    "\n",
    "So far, we used `wget` to download from URLs. There is another command, `curl`, that also downloads from URLs. In this assignment, we will use `curl` and other command line tools to scrape [annual air quality summary data from EPA](https://aqs.epa.gov/aqsweb/airdata/download_files.html#Annual).\n",
    "\n",
    "![aqi-chart](aqi-chart.png)\n",
    "\n",
    "* Visit and [inspect the source code](https://www.lifewire.com/view-web-source-code-4151702) of this EPA webpage: https://aqs.epa.gov/aqsweb/airdata/download_files.html\n",
    "* Use `curl` command to download the same URL and [save the output to a file using `>` redirect](http://swcarpentry.github.io/shell-novice/04-pipefilter/index.html). Name the output file `files.html`\n",
    "* Use `tail` to print 10 last lines of `files.html`\n",
    "\n",
    "_Use `!` or `%%bash` when running any shell command inside the notebook, and replace `# use [command] here` with your answer_\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "manual: true\n",
    "points: 3\n",
    "gradescope: hide\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! # use curl here\n",
    "! # use tail here\n",
    "! tail -n10 files.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1b: Chaining commands\n",
    "\n",
    "Instead of saving the output to file, then printing the last ten lines, you can execute a [sequence of commands together using pipes](http://swcarpentry.github.io/shell-novice/04-pipefilter/index.html). Using pipes, write a one line shell command to replace Question 1a.\n",
    "\n",
    "You can see from the output that the download progress is printed to screen. Something like this:\n",
    "```\n",
    "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
    "                                 Dload  Upload   Total   Spent    Left  Speed\n",
    "100  307k  100  307k    0     0   319k      0 --:--:-- --:--:-- --:--:--  319k\n",
    "```\n",
    "We don't want this to be a part of the html code, so use `curl`'s silent mode to eliminate the progress information.\n",
    "\n",
    "Save the result to a python variable named `lastlines`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: hide\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastlines = ! # one line pipe command here\n",
    "print('\\n'.join(lastlines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 2: Text Processing\n",
    "\n",
    "## Question 2a: `grep`\n",
    "\n",
    "The `grep` command searches lines in a text file. Inspect what kinds of options are available using the help page: `grep --help`.\n",
    "\n",
    "Use `grep` to find lines with the substring `zip` and print first five lines using `head`. To avoid downloading the html page over and over again, use `files.html` as your input to `grep`.\n",
    "\n",
    "Save the resulting list of strings to a python variable named `ziplines`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: hide\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ziplines = ! # your command\n",
    "print('\\n'.join(ziplines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2b: Regular expression\n",
    "\n",
    "Note how the word `zip` doesn't just appear as a part of zip file names. We want our `grep` to match more specific pattern that looks like zip file names. The part of the html that becomes the download link is [`href`](https://www.w3schools.com/tags/tryit.asp?filename=tryhtml_a_href). Let's search for filenames specified by `href`. \n",
    "\n",
    "Take, for example, the following line:\n",
    "```\n",
    "<a href=\"aqs_monitors.zip\">Monitor Listing</a> (353,283 Rows, 6,307 KB)<BR><BR>\n",
    "```\n",
    "There are many different and better ways of doing this. We will use `grep` and regular expression patterns to extract the links. A good way to learn regular expression is to know the basics and experiment.\n",
    "\n",
    "* [Quick reference of regular expression pattern](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) showing the basic building blocks\n",
    "* [Online utility for testing regular expression patterns](https://regexr.com)\n",
    "\n",
    "One quick way could be to find the pattern `href=\"[some file name]` to find all substrings containing links. Then filter the lines again that contain the word `zip`.\n",
    "\n",
    "Follow this sequence of commands using pipes:\n",
    "1. **Find lines with substring`zip` using `grep` and `files.html`.**  \n",
    "    First few lines would look similar to,\n",
    "```\n",
    "    the size of the (zipped) file, the number of data rows in the file,\n",
    "<a href=\"aqs_sites.zip\">Site Listing</a> (20,611 Rows, 982 KB)<BR>\n",
    "<a href=\"aqs_monitors.zip\">Monitor Listing</a> (353,283 Rows, 6,307 KB)<BR><BR>\n",
    "            <TD style=\"font-size:90%;text-align:center;\"><a href=\"annual_conc_by_monitor_2019.zip\">annual_conc_by_monitor_2019.zip</a><BR>51,707 Rows<BR>3,081 KB<BR>As of 2019-11-13</TD>\n",
    "```\n",
    "1. **Find lines with `grep` and a [regular expression](http://swcarpentry.github.io/shell-novice/07-find/index.html) pattern that matches `href=\"....`.**  \n",
    "    _When using `grep` with regular expressions, it is easier to use `-E` option (for extended regular expression)._ After using `grep` option for extracting just the matching substrings, you would get something similar to this:\n",
    "```\n",
    "href=\"aqs_sites.zip\n",
    "href=\"aqs_monitors.zip\n",
    "href=\"annual_conc_by_monitor_2019.zip\n",
    "href=\"annual_aqi_by_cbsa_2019.zip\n",
    "href=\"annual_aqi_by_county_2019.zip    \n",
    "```\n",
    "1. **Strip `href=\"` by using `sed`**  \n",
    "    Test using something like `echo \"hello there\" | sed 's/ll/rr/'`. After running `sed`, you would get something like this:\n",
    "```\n",
    "aqs_sites.zip\n",
    "aqs_monitors.zip\n",
    "annual_conc_by_monitor_2019.zip\n",
    "annual_aqi_by_cbsa_2019.zip\n",
    "annual_aqi_by_county_2019.zip    \n",
    "```\n",
    "    [_Refer to Unix Power Tools for more information (UCSB NetID required)_](https://proquest-safaribooksonline-com.proxy.library.ucsb.edu:9443/book/operating-systems-and-server-administration/unix/0596003307/34dot-the-sed-stream-editor/upt3_chp_34_sect_3_html)\n",
    "1. **Save the resulting list of strings (each string is a URL for a zip file) a python variable named `flist`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: hide\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = ! grep ... files.html \\\n",
    "#    | grep ... 'some-pattern' \\\n",
    "#    | sed ...\n",
    "print('\\n'.join(flist[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2c: Double check\n",
    "\n",
    "Up to 2b, we created a list of files by scraping the website: https://aqs.epa.gov/aqsweb/airdata/download_files.html. This list of files is in the python variable `flist` in 2b.\n",
    "\n",
    "It happens that the same website provides a complete list of files in a csv file at https://aqs.epa.gov/aqsweb/airdata/file_list.csv. We want to double check our work by comparing the list of files between our approach and this list of files. Using a method of your choosing, find out what accounts for the difference in file counts.\n",
    "\n",
    "One possible way is to, first, take `file_list.csv`, extract the filename column, subset just the zip filenames, sort alphabetically, and create the resulting file in `flist1.txt`. Then, create a separate file `flist2.txt` from output from previous question. Finally, use `diff` to compare files `flist1.txt` and `flist2.txt`.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "manual: true\n",
    "points: 5\n",
    "gradescope: hide\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "! # create flist1.txt\n",
    "! # create flist2.txt\n",
    "! diff flist1.txt flist2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Download `https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2019.zip` and unzip to extract `annual_aqi_by_county_2019.csv`.\n",
    "\n",
    "## Question 3a: Inspecting the Header\n",
    "\n",
    "Use `head` to print the header line of `annual_aqi_by_county_2019.csv` and use `sed` to print each column into one line. You can replace commas (`,`) with a newline character (`\\n`). Save to a python list variable named `aqiheader`. Each header name will become a list element.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: hide\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqiheader = ! # your command\n",
    "print('\\n'.join(aqiheader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3b: Count Locations\n",
    "\n",
    "How many counties are there in each state? Each row is a county, so you can extract the `State` column and count how many rows there are for each state using `uniq`. Then, use `cat` to number the output lines. Save the result to a python variable `county_counts`. \n",
    "\n",
    "First few lines should look like this:\n",
    "```\n",
    "   1\t     16 \"Alabama\"\n",
    "   2\t      6 \"Alaska\"\n",
    "   3\t     13 \"Arizona\"\n",
    "   4\t     13 \"Arkansas\"\n",
    "   5\t     53 \"California\"\n",
    "```\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: hide\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "county_counts = ! # your command\n",
    "print('\\n'.join(county_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3c: View CSV in Table\n",
    "\n",
    "Take first 5 lines of `annual_aqi_by_county_2019.csv`, extract first seven columns, and use `column` command to view data in a table. Assign the result to a python variable named `aqi_table`. The result looks like this:\n",
    "```\n",
    "\"State\"    \"County\"   \"Year\"  \"Days with AQI\"  \"Good Days\"  \"Moderate Days\"  \"Unhealthy for Sensitive Groups Days\"\n",
    "\"Alabama\"  \"Baldwin\"  2019    166              140          26               0\n",
    "\"Alabama\"  \"Clay\"     2019    63               58           5                0\n",
    "\"Alabama\"  \"Colbert\"  2019    171              161          10               0\n",
    "\"Alabama\"  \"DeKalb\"   2019    208              188          20               0\n",
    "```\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3c\n",
    "manual: false\n",
    "points: 5\n",
    "gradescope: hide\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_table = ! # your command\n",
    "print('\\n'.join(aqi_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Question 4\n",
    "\n",
    "# Question 4a: Reading data in\n",
    "\n",
    "All zip filenames were stored in a variable named `flist`. Let's read annual AQI by county data from _all_ years and create a big data frame.\n",
    "Create an empty data frame, `allaqi = pd.DataFrame()`\n",
    "\n",
    "Now, we build up `allaqi` year by year:\n",
    "* By looping through `flist` variable, for each zip filename,\n",
    "* Check if the zip file name is the right format for annual AQI data: i.e. starts with `annual_aqi_by_county`\n",
    "* If so, download it from a URL constructed by adding `https://aqs.epa.gov/aqsweb/airdata/` to the filename.  \n",
    "    _Hint 1: dissect what is happening in the following code example_  \n",
    "    ![python-variable-to-shell](pyvar-to-shell.png)  \n",
    "    _Hint 2: skip download if the file is already there by using option `wget -nc`_  \n",
    "* Read it into a pandas data frame, `tmpdf`. _Tip: `pandas.read_csv` can read zipped files directly. See `compression` in documentation for `pandas.read_csv`_\n",
    "* Concatenate to `allaqi` by using `pd.concat`\n",
    "\n",
    "Finally, convert `Year` column to datetime with `pandas.to_datetime`, and set index to `['State', 'County']` with`pd.DataFrame.set_index`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4a\n",
    "manual: false\n",
    "points: 10\n",
    "gradescope: show\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "allaqi = pd.DataFrame()\n",
    "for f in flist:\n",
    "    if [some-pattern] in f:\n",
    "        ! # download command\n",
    "        tmpdf = ... # read \n",
    "\n",
    "# Convert Year to datetime\n",
    "...\n",
    "allaqi = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4b: Subsetting Data\n",
    "\n",
    "We will find counties with worst AQI per state. First, filter counties with 30 years worth of data (1989-2018) whose AQI was calculated from at least 300 observations per year. Subsequently, if there is more than one in a state that qualifies, pick the county with higher average `Median AQI` over all the years.\n",
    "\n",
    "### DataFrame `fullaqi`\n",
    "\n",
    "* To subset the columns to:\n",
    "```\n",
    "['Days with AQI','Good Days','Unhealthy Days','Max AQI','90th Percentile AQI','Median AQI']\n",
    "```\n",
    "* Group by State and County and \n",
    "* Keep only those counties that have full data (30 years) between 1989 and 2018 **and** all years have at least 300 `Days with AQI`\n",
    "* Sort the index\n",
    "\n",
    "### DataFrame `worst_counties`\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4b\n",
    "manual: false\n",
    "points: 10\n",
    "gradescope: show\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Year','Days with AQI','Good Days','Unhealthy Days','Max AQI','90th Percentile AQI','Median AQI']\n",
    "filt = lambda x: len(x) == 30 and x['Days with AQI'].min() >= 300 # filter\n",
    "fullaqi = allaqi[cols]\\\n",
    "    .query(...)\\        # between years 1989 and 2018 \n",
    "    .groupby(...)\\      # State and County\n",
    "    .filter(...)\\       # every annual measurement and at least 300 days measured per year\n",
    "    .sort_index()\n",
    "worst_counties = fullaqi\\         \n",
    "    .groupby(...)\\      # State and County\n",
    "    .agg(...)\\          # mean of Median AQI\n",
    "    .groupby(...)\\      # State\n",
    "    .agg(...)\\          # lambda function returning idxmax\n",
    "    .values.flatten()    \n",
    "worstaqi = fullaqi.loc[worst_counties].sort_index().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4c: Visualization\n",
    "\n",
    "We will visualize the wrangled data. In `worstaqi`, there are many counties, so we will use Altair to help us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "highlight = alt.selection(type='single', on='mouseover', fields=['State'], nearest=True)\n",
    "\n",
    "base = alt.Chart(worstaqi).transform_calculate(\n",
    "    Location=\"datum.County + ', ' + datum.State\"\n",
    ").encode(\n",
    "    x='Year:T',\n",
    "    y='Median AQI:Q',\n",
    "    color='Location:N',\n",
    ")\n",
    "\n",
    "\n",
    "points = base\\\n",
    "    .mark_circle()\\\n",
    "    .encode(\n",
    "        opacity=alt.value(0),\n",
    "        tooltip=['Location:N', 'year(Year)'])\\\n",
    "    .add_selection(highlight)\\\n",
    "    .properties(\n",
    "        width=600,\n",
    "        height=400)\n",
    "\n",
    "lines = base.mark_line().encode(\n",
    "    size=alt.condition(~highlight, alt.value(1), alt.value(3)),\n",
    "    opacity=alt.condition(~highlight, alt.value(0.7), alt.value(1))\n",
    ")\n",
    "\n",
    "points + lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Visualize Annual AQI Changes\n",
    "\n",
    "Calculate the annual relative change of Median AQI. Define Annual relative change of Median AQI for 2017 as,\n",
    "$$\\text{Relative Change of MAQI}_{2017} = \\frac{\\text{MAQI}_{2018} - \\text{MAQI}_{2017}}{\\text{MAQI}_{2017}} = \\frac{51-55}{55} \\approx -0.07272$$\n",
    "You can use `pandas.DataFrame.apply()` to achieve this. \n",
    "\n",
    "For example,\n",
    "![Relative AQI Change](aqichange.png)\n",
    "\n",
    "Name the data frame `diffworstaqi`. Create the same kind of plot as above.\n",
    "Qualitatively, what can you conclude from this plot? Can you create a different plot to illustrate your point?\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4c\n",
    "manual: true\n",
    "points: 10\n",
    "gradescope: show\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "diffworstaqi = \\\n",
    "    worstaqi\\\n",
    "    .set_index(['State','County','Year'])[['Median AQI']]\\  # just the column we need\n",
    "    .sort_index(...)\\                                       # ensure year column is decreasing\n",
    "    .groupby(['State', 'County'])\\                          # for each (state, county),\n",
    "    .apply(lambda x: -x.diff()/x)\\                          # calculate relative change\n",
    "    .reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Running Built-in Tests\n",
    "1. All tests are in `tests` directory\n",
    "1. Each python file in `tests` is a test\n",
    "1. `grader.check('testname')` runs test `'testname'`, e.g. `'q1'`\n",
    "1. `grader.check_all()` runs all visible tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run built-in checks\n",
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate pdf in classic notebook (does not work in JupyterLab):\n",
    "# import nb2pdf\n",
    "# nb2pdf.convert('assignment2.ipynb')\n",
    "\n",
    "# Uncomment to generate pdf using command-line tool:\n",
    "# ! nb2pdf assignment2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submission Checklist\n",
    "1. Check filename is `assignment2.ipynb`\n",
    "1. Save file to confirm all changes are on disk\n",
    "1. Run *Kernel > Restart & Run All* to execute all code from top to bottom\n",
    "1. Check `grader.check_all()` output\n",
    "1. Save file again to write any new output to disk\n",
    "1. Check generated pdf that all responses are displayed correctly\n",
    "1. Submit `assignment2.ipynb` and `assignment2.pdf` to Gradescope"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
